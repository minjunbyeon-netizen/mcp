#!/usr/bin/env python3
"""
í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ê¸°
ì‚¬ìš©ë²•: python run_blog_generator.py
"""

import sys
import os
import json
import io
import threading
import time
import subprocess
from datetime import datetime
from pathlib import Path

# Windows í„°ë¯¸ë„ UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# mcp_config.jsonì—ì„œ API í‚¤ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

config_path = Path(__file__).parent / "mcp_config.json"
if config_path.exists() and not os.getenv("ANTHROPIC_API_KEY"):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
        api_key = config.get("mcpServers", {}).get("content-automation", {}).get("env", {}).get("ANTHROPIC_API_KEY")
        if api_key:
            os.environ["ANTHROPIC_API_KEY"] = api_key

import anthropic
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH


class LoadingSpinner:
    """ë¡œë”© ìŠ¤í”¼ë„ˆ ì• ë‹ˆë©”ì´ì…˜"""
    def __init__(self, message="ì²˜ë¦¬ ì¤‘"):
        self.message = message
        self.running = False
        self.thread = None
    
    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._animate)
        self.thread.start()
    
    def _animate(self):
        frames = ['|', '/', '-', '\\']
        i = 0
        while self.running:
            print(f"\r  {frames[i % 4]} {self.message}...", end="", flush=True)
            time.sleep(0.2)
            i += 1
    
    def stop(self, success_msg="ì™„ë£Œ"):
        self.running = False
        if self.thread:
            self.thread.join()
        print(f"\r  [OK] {success_msg}" + " " * 20)

# ê²½ë¡œ ì„¤ì •
PERSONA_DIR = Path(__file__).parent / "output" / "personas"
PERSONA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR = Path.home() / "mcp-data" / "outputs"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Word íŒŒì¼ ì „ìš© ì €ì¥ ìœ„ì¹˜
WORD_OUTPUT_DIR = Path(__file__).parent / "output" / "blog"
WORD_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ì…ë ¥ í´ë” (ë³´ë„ìë£Œ í…ìŠ¤íŠ¸ íŒŒì¼ ë„£ëŠ” ê³³)
INPUT_DIR = Path(__file__).parent / "input" / "press_release"
INPUT_DIR.mkdir(parents=True, exist_ok=True)


def list_personas():
    """ì €ì¥ëœ í˜ë¥´ì†Œë‚˜ ëª©ë¡"""
    personas = []
    for file_path in PERSONA_DIR.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if "client_id" in data and "persona_analysis" in data:
                    personas.append({
                        "client_id": data["client_id"],
                        "client_name": data["client_name"],
                        "organization": data["organization"],
                        "formality": data["persona_analysis"]["formality_level"]["score"]
                    })
        except:
            pass
    return personas


def generate_blog_post(client_id: str, press_release: str, target_keywords: list = None):
    """ë¸”ë¡œê·¸ ê¸€ ìƒì„±"""
    
    print(f"\n{'='*50}")
    print(f"  AI ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹œì‘")
    print(f"{'='*50}")
    
    # í˜ë¥´ì†Œë‚˜ ë¡œë“œ
    persona_path = PERSONA_DIR / f"{client_id}.json"
    if not persona_path.exists():
        print(f"âŒ í˜ë¥´ì†Œë‚˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {client_id}")
        return None
    
    with open(persona_path, 'r', encoding='utf-8') as f:
        persona_data = json.load(f)
    
    custom_prompt = persona_data["custom_prompt"]
    client_name = persona_data["client_name"]
    
    print(f"  í˜ë¥´ì†Œë‚˜: {client_name}")
    print(f"  ë³´ë„ìë£Œ: {len(press_release):,} ê¸€ì")
    print(f"{'='*50}\n")
    
    # Step 1: API ì—°ê²°
    print("[1/3] API ì—°ê²° ì¤€ë¹„")
    spinner = LoadingSpinner("Claude AI ì—°ê²° ì¤‘")
    spinner.start()
    time.sleep(0.5)
    client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    spinner.stop("API ì—°ê²° ì™„ë£Œ")
    
    # ë¸”ë¡œê·¸ ê¸€ ìƒì„± í”„ë¡¬í”„íŠ¸ (ë¶€ì‚°ì‹œ ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼)
    keywords_str = ", ".join(target_keywords) if target_keywords else ""
    
    blog_prompt = f"""
{{
  "system_settings": {{
    "role": "ë¶€ì‚°ì‹œ ê³µì‹ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì—ë””í„° (í˜ë¥´ì†Œë‚˜ ì¼ì¹˜ìœ¨ 100% ëª©í‘œ)",
    "objective": "ë”±ë”±í•œ ë³´ë„ìë£Œë¥¼ 'ë¶€ì‚°ì‹œ ë¸”ë¡œê·¸ ê³ ìœ ì˜ ì¹œê·¼í•˜ê³  ìƒëƒ¥í•œ ìŠ¤íƒ€ì¼'ë¡œ ì™„ë²½í•˜ê²Œ ë³€í™˜",
    "persona_enforcement_level": "CRITICAL (ì´ ê°€ì´ë“œë¥¼ ë”°ë¥´ì§€ ì•Šì„ ê²½ìš° ì˜¤ë‹µìœ¼ë¡œ ê°„ì£¼í•¨)"
  }},
  "input_context": {{
    "press_release": "{press_release}",
    "target_keywords": ["{keywords_str}"],
    "custom_request": "{custom_prompt}"
  }},
  "strict_persona_guide": {{
    "tone_and_manner": {{
      "primary_emotion": "ì¹œì ˆí•¨, ë”°ëœ»í•¨, ìë¶€ì‹¬, ê¸ì •ì  ì—ë„ˆì§€",
      "sentence_ending_rule": "í•´ìš”ì²´(~ì¸ë°ìš”, ~ì¸ë°ìš”!) 70% + í•©ì‡¼ì²´(~ìŠµë‹ˆë‹¤) 30% ë¹„ìœ¨ ìœ ì§€. (ì ˆëŒ€ ë”±ë”±í•œ 'í•œë‹¤'ì²´ ê¸ˆì§€)",
      "mandatory_punctuation": [
        "ë¬¸ì¥ ëì„ ë¶€ë“œëŸ½ê²Œ ë§ºëŠ” ë¬¼ê²°í‘œ(~) í•„ìˆ˜ ì‚¬ìš© (ì˜ˆ: ì•„ì‹¤ í…ë°ìš”~, ì˜ˆì •ì¸ë°ìš”~)",
        "ê°ì •ì„ ì‹£ëŠ” ëŠë‚Œí‘œ(!) ì‚¬ìš©",
        "ì„œë¡ ê³¼ ê²°ë¡ ì— ì¹œê·¼í•œ ì´ëª¨í‹°ì½˜ ( :), ğŸ˜€ ) ë°°ì¹˜"
      ],
      "banned_styles": [
        "ê¸°ê³„ì ì¸ ë²ˆì—­íˆ¬",
        "ì§€ë‚˜ì¹˜ê²Œ ê±´ì¡°í•œ ê°œì¡°ì‹ ë‚˜ì—´",
        "ì–´ë µê³  ê¶Œìœ„ì ì¸ í–‰ì • ìš©ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë°˜ë“œì‹œ ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì“¸ ê²ƒ)"
      ],
      "banned_characters": [
        "ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ ê¸ˆì§€: " " ' ' ëŒ€ì‹  ì¼ë°˜ ë”°ì˜´í‘œ ì‚¬ìš©",
        "ë§ì¤„ì„í‘œ ê¸ˆì§€: ... ë˜ëŠ” â€¦ ì‚¬ìš© ê¸ˆì§€",
        "ë¶ˆí•„ìš”í•œ ìƒëµ í‘œí˜„ ê¸ˆì§€"
      ]
    }},
    "visual_formatting_rules": {{
      "header_style": "ì†Œì œëª©ì€ ë°˜ë“œì‹œ ã€Œ êº½ì‡  ê´„í˜¸ ã€ ì•ˆì— í‚¤ì›Œë“œë¡œ ì‘ì„± (ì˜ˆ: ã€Œ 15ë¶„ ë„ì‹œ ã€)",
      "emphasis_style": "í•µì‹¬ í˜œíƒ, ìˆ«ì, ëª©í‘œ ì‹œê¸°ëŠ” ë°˜ë“œì‹œ **êµµê²Œ(Bold)** ì²˜ë¦¬",
      "layout_style": "ê°€ë…ì„±ì„ ìœ„í•´ 3~4ì¤„ë§ˆë‹¤ ì¤„ë°”ê¿ˆ(Enter) í•„ìˆ˜, ì„¹ì…˜ ê°„ êµ¬ë¶„ì„ (â€¢ â€¢ â€¢ â€¢ â€¢) ì‚¬ìš©",
      "image_placeholder": "ê° ì„¹ì…˜(ì†Œì œëª©) ì•„ë˜ì— [ì´ë¯¸ì§€] ìë¦¬ í‘œì‹œ ì‚½ì… (ì˜ˆ: [ì´ë¯¸ì§€: í•œì¤‘ MOU ì²´ê²° í˜„ì¥])"
    }}
  }},
  "content_structure_blueprint": {{
    "intro": {{
      "hook_question": "ë…ìì—ê²Œ ë§ì„ ê±°ëŠ” ì§ˆë¬¸í˜• ì‹œì‘ (ì˜ˆ: '~~ ì†Œì‹, ì•Œê³  ê³„ì‹œë‚˜ìš”?')",
      "bridge": "ëª¨ë¥´ëŠ” ë¶„ë“¤ì„ ìœ„í•´ í•µì‹¬ë§Œ ì •ë¦¬í–ˆë‹¤ëŠ” ì¹œì ˆí•œ ì•ˆë‚´ ë©˜íŠ¸"
    }},
    "body": {{
      "flow": "ì†Œì œëª©(í‚¤ì›Œë“œ) -> í˜„í™© ì„¤ëª…(ì¹œê·¼í•˜ê²Œ) -> **í•µì‹¬ ë‚´ìš©/í˜œíƒ ê°•ì¡°** -> í–¥í›„ ê³„íš",
      "narrative": "ë³´ë„ìë£Œì˜ íŒ©íŠ¸ë¥¼ ì „ë‹¬í•˜ë˜, 'ì‹œë¯¼ì˜ ì…ì¥ì—ì„œ ì´ê²Œ ì™œ ì¢‹ì€ì§€'ë¥¼ ì„¤ëª…í•˜ëŠ” í™”ë²• ì‚¬ìš©"
    }},
    "outro": {{
      "closing": "ë‚´ìš© ìš”ì•½ ë° ì•ìœ¼ë¡œë„ ì†Œì‹ì„ ë¹ ë¥´ê²Œ ì „í•˜ê² ë‹¤ëŠ” ì•½ì†",
      "cta": "ê´€ì‹¬ê³¼ ì§€ì¼œë´ ë‹¬ë¼ëŠ” ë‹¹ë¶€ + ì´ëª¨í‹°ì½˜(ğŸ˜€)ìœ¼ë¡œ ë§ˆë¬´ë¦¬"
    }}
  }},
  "task_requirements": {{
    "seo_optimization": {{
      "title": "ê³µë°± í¬í•¨ 60ì ì´ë‚´, í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë§¤ë ¥ì ì¸ ì œëª©, í‚¤ì›Œë“œ í¬í•¨",
      "meta_description": "155ì ì´ë‚´, ê²€ìƒ‰ ê²°ê³¼ ë…¸ì¶œìš© ìš”ì•½",
      "keyword_integration": "ì œê³µëœ í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì— 3íšŒ ì´ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚¼ ê²ƒ"
    }},
    "length": "ê³µë°± í¬í•¨ 1,500 ~ 2,000ì (ë‚´ìš©ì„ í’ì„±í•˜ê²Œ ëŠ˜ë ¤ì„œ ì‘ì„±)"
  }},
  "output_schema": {{
    "description": "ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œë§Œ ì¶œë ¥í•  ê²ƒ (Markdown ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ì—)",
    "format": {{
      "title": "ë¸”ë¡œê·¸ ì œëª© String",
      "content": "HTML íƒœê·¸ ì—†ì´ Markdown í˜•ì‹ì´ ì ìš©ëœ ë³¸ë¬¸ String",
      "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3", "íƒœê·¸4", "íƒœê·¸5"],
      "meta_description": "ë©”íƒ€ ì„¤ëª… String"
    }}
  }}
}}
"""
    
    # Step 2: AI ë¸”ë¡œê·¸ ìƒì„±
    print("\n[2/3] ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì¤‘")
    spinner = LoadingSpinner("AIê°€ í˜ë¥´ì†Œë‚˜ ìŠ¤íƒ€ì¼ë¡œ ê¸€ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤")
    spinner.start()
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=3000,
            messages=[{"role": "user", "content": blog_prompt}]
        )
        spinner.stop("ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì™„ë£Œ")
        
        # Step 3: ê²°ê³¼ ì²˜ë¦¬
        print("\n[3/3] íŒŒì¼ ì €ì¥ ì¤‘")
        spinner = LoadingSpinner("Word/Markdown íŒŒì¼ ìƒì„± ì¤‘")
        spinner.start()
        
        response_text = response.content[0].text
        
        # JSON ì¶”ì¶œ
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0]
        
        blog_content = json.loads(response_text.strip())
        
    except Exception as e:
        spinner.stop("ì˜¤ë¥˜ ë°œìƒ")
        print(f"\nâŒ ë¸”ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
    # ì €ì¥
    output_id = f"BLOG_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    blog_data = {
        "output_id": output_id,
        "client_id": client_id,
        "client_name": client_name,
        "type": "blog",
        "content": blog_content,
        "created_at": datetime.now().isoformat()
    }
    
    # JSON ì €ì¥
    json_path = OUTPUT_DIR / f"{output_id}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(blog_data, f, ensure_ascii=False, indent=2)
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë„ ìƒì„±
    md_path = OUTPUT_DIR / f"{output_id}.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# {blog_content['title']}\n\n")
        f.write(f"{blog_content['content']}\n\n")
        f.write(f"**íƒœê·¸:** {', '.join(blog_content['tags'])}\n")
    
    # Word íŒŒì¼ ìƒì„± (ë³„ë„ ìœ„ì¹˜ì— í˜ë¥´ì†Œë‚˜ëª…_ì œëª©_ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ)
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    safe_title = blog_content['title'][:30].replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_').strip()
    safe_client_name = client_name.replace(' ', '_')
    date_str = datetime.now().strftime('%Y%m%d')
    docx_filename = f"{safe_client_name}_{safe_title}_{date_str}.docx"
    docx_path = WORD_OUTPUT_DIR / docx_filename
    doc = Document()
    
    # ê¸°ë³¸ ìŠ¤íƒ€ì¼ì— í•œê¸€ í°íŠ¸ ì„¤ì •
    style = doc.styles['Normal']
    font = style.font
    font.name = 'ë§‘ì€ ê³ ë”•'
    font.size = Pt(11)
    
    # ì œëª© ì¶”ê°€
    title = doc.add_heading(blog_content['title'], level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    # ì œëª©ì—ë„ í•œê¸€ í°íŠ¸ ì ìš©
    for run in title.runs:
        run.font.name = 'ë§‘ì€ ê³ ë”•'
    
    # ë³¸ë¬¸ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê°„ì†Œí™”)
    content = blog_content['content']
    paragraphs = content.split('\n\n')
    
    for para in paragraphs:
        if para.strip():
            # ì†Œì œëª© ì²˜ë¦¬ (ã€Œ ã€)
            if para.strip().startswith('ã€Œ') and para.strip().endswith('ã€'):
                p = doc.add_heading(para.strip()[1:-1].strip(), level=2)
                for run in p.runs:
                    run.font.name = 'ë§‘ì€ ê³ ë”•'
            # êµ¬ë¶„ì„  ì²˜ë¦¬
            elif para.strip() == 'â€¢ â€¢ â€¢ â€¢ â€¢':
                p = doc.add_paragraph('â”€' * 30)
                p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                for run in p.runs:
                    run.font.name = 'ë§‘ì€ ê³ ë”•'
            # ì´ë¯¸ì§€ ìë¦¬ í‘œì‹œ
            elif para.strip().startswith('[ì´ë¯¸ì§€'):
                p = doc.add_paragraph(para.strip())
                p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                run = p.runs[0]
                run.italic = True
                run.font.name = 'ë§‘ì€ ê³ ë”•'
            else:
                # ì¼ë°˜ ë³¸ë¬¸ - **êµµê²Œ** ì²˜ë¦¬
                p = doc.add_paragraph()
                parts = para.split('**')
                for i, part in enumerate(parts):
                    run = p.add_run(part)
                    run.font.name = 'ë§‘ì€ ê³ ë”•'
                    run.font.size = Pt(11)
                    if i % 2 == 1:  # í™€ìˆ˜ ì¸ë±ìŠ¤ëŠ” êµµê²Œ
                        run.bold = True
    
    # íƒœê·¸ ì¶”ê°€
    doc.add_paragraph()
    tags_para = doc.add_paragraph()
    tags_run = tags_para.add_run(f"íƒœê·¸: {', '.join(blog_content['tags'])}")
    tags_run.italic = True
    tags_run.font.name = 'ë§‘ì€ ê³ ë”•'
    
    # ë©”íƒ€ ì„¤ëª… ì¶”ê°€
    meta_para = doc.add_paragraph()
    meta_run = meta_para.add_run(f"ë©”íƒ€ ì„¤ëª…: {blog_content['meta_description']}")
    meta_run.italic = True
    meta_run.font.name = 'ë§‘ì€ ê³ ë”•'
    
    doc.save(str(docx_path))
    spinner.stop("íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    
    return blog_data, md_path, docx_path


def generate_blog_with_persona(client_id: str):
    """í˜ë¥´ì†Œë‚˜ ì¶”ì¶œ í›„ ë°”ë¡œ ë¸”ë¡œê·¸ ìƒì„± (ì—°ê³„ í˜¸ì¶œìš©)"""
    print("\n" + "=" * 60)
    print("ğŸ“ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ê¸°")
    print("=" * 60)
    
    # ë³´ë„ìë£Œ í´ë” ìŠ¤ìº”
    press_files = [f for f in INPUT_DIR.glob("*.txt") if f.name.lower() != "readme.txt"]
    
    if not press_files:
        print("\nâŒ ë³´ë„ìë£Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ğŸ“‚ ì´ í´ë”ì— .txt íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”:")
        print(f"   {INPUT_DIR}")
        return
    
    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
    print("\nğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ë„ìë£Œ:")
    print("-" * 50)
    for i, f in enumerate(press_files, 1):
        size_kb = f.stat().st_size / 1024
        print(f"  {i}. {f.stem}")
        print(f"     ({f.name}, {size_kb:.1f}KB)")
    
    # ë²ˆí˜¸ë¡œ ì„ íƒ
    print("\nğŸ”¢ ì‚¬ìš©í•  ë³´ë„ìë£Œ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    try:
        choice = int(input(">>> ").strip())
        if choice < 1 or choice > len(press_files):
            print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
            return
        selected_file = press_files[choice - 1]
    except ValueError:
        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"\nâœ… ì„ íƒ: {selected_file.name}")
    
    # íŒŒì¼ ì½ê¸°
    with open(selected_file, 'r', encoding='utf-8') as f:
        press_release = f.read()
    
    print(f"ğŸ“„ ë³´ë„ìë£Œ ê¸¸ì´: {len(press_release):,} ê¸€ì")
    
    # SEO í‚¤ì›Œë“œ (ì„ íƒ)
    print("\nğŸ”‘ SEO í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„, ì—†ìœ¼ë©´ ì—”í„°):")
    keywords_input = input(">>> ").strip()
    keywords = [k.strip() for k in keywords_input.split(",")] if keywords_input else None
    
    # ë¸”ë¡œê·¸ ìƒì„±
    result = generate_blog_post(client_id, press_release, keywords)
    
    if result:
        blog_data, md_path, docx_path = result
        blog = blog_data["content"]
        
        print("\n" + "=" * 60)
        print("âœ… ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì™„ë£Œ!")
        print("=" * 60)
        
        print(f"\nğŸ“Œ ì œëª©: {blog['title']}")
        print(f"ğŸ·ï¸ íƒœê·¸: {', '.join(blog['tags'])}")
        print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜:")
        print(f"   - Word: {docx_path}")
        
        # í´ë” ì—´ê¸° ì˜µì…˜
        print("\n" + "=" * 60)
        print("ğŸ“‚ ë¸”ë¡œê·¸ í´ë”ë¥¼ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ", end="")
        open_folder = input().strip().lower()
        if open_folder != 'n':
            subprocess.run(['explorer', str(WORD_OUTPUT_DIR)])
            print("   í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ë¸”ë¡œê·¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


def main():
    print("=" * 60)
    print("ğŸ“ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ê¸°")
    print("=" * 60)
    
    # í˜ë¥´ì†Œë‚˜ ëª©ë¡ í‘œì‹œ
    personas = list_personas()
    if not personas:
        print("\nâŒ ì €ì¥ëœ í˜ë¥´ì†Œë‚˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € run_persona_test.pyë¡œ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return
    
    print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜:")
    print("-" * 50)
    for i, p in enumerate(personas, 1):
        print(f"  {i}. {p['client_name']}")
        print(f"     ({p['organization']}) - ê²©ì‹ë„: {p['formality']}/10")
    
    # í˜ë¥´ì†Œë‚˜ ì„ íƒ
    print("\nğŸ”¢ ì‚¬ìš©í•  í˜ë¥´ì†Œë‚˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    try:
        choice = int(input(">>> ").strip())
        selected = personas[choice - 1]
        client_id = selected["client_id"]
    except:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì„ íƒëœ í˜ë¥´ì†Œë‚˜: {selected['client_name']}")
    
    # ë³´ë„ìë£Œ í´ë” ìŠ¤ìº”
    press_files = [f for f in INPUT_DIR.glob("*.txt") if f.name.lower() != "readme.txt"]
    
    if not press_files:
        print("\nâŒ ë³´ë„ìë£Œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ğŸ“‚ ì´ í´ë”ì— .txt íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”:")
        print(f"   {INPUT_DIR}")
        return
    
    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
    print("\nğŸ“‚ ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ë„ìë£Œ:")
    print("-" * 50)
    for i, f in enumerate(press_files, 1):
        size_kb = f.stat().st_size / 1024
        print(f"  {i}. {f.stem}")
        print(f"     ({f.name}, {size_kb:.1f}KB)")
    
    # ë²ˆí˜¸ë¡œ ì„ íƒ
    print("\nğŸ”¢ ì‚¬ìš©í•  ë³´ë„ìë£Œ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    try:
        choice = int(input(">>> ").strip())
        if choice < 1 or choice > len(press_files):
            print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
            return
        selected_file = press_files[choice - 1]
    except ValueError:
        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"\nâœ… ì„ íƒ: {selected_file.name}")
    
    # íŒŒì¼ ì½ê¸°
    with open(selected_file, 'r', encoding='utf-8') as f:
        press_release = f.read()
    
    print(f"ğŸ“„ ë³´ë„ìë£Œ ê¸¸ì´: {len(press_release):,} ê¸€ì")
    
    # SEO í‚¤ì›Œë“œ (ì„ íƒ)
    print("\nğŸ”‘ SEO í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„, ì—†ìœ¼ë©´ ì—”í„°):")
    keywords_input = input(">>> ").strip()
    keywords = [k.strip() for k in keywords_input.split(",")] if keywords_input else None
    
    # ë¸”ë¡œê·¸ ìƒì„±
    result = generate_blog_post(client_id, press_release, keywords)
    
    if result:
        blog_data, md_path, docx_path = result
        blog = blog_data["content"]
        
        print("\n" + "=" * 60)
        print("âœ… ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì™„ë£Œ!")
        print("=" * 60)
        
        print(f"\nğŸ“Œ ì œëª©: {blog['title']}")
        print(f"ğŸ·ï¸ íƒœê·¸: {', '.join(blog['tags'])}")
        print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜:")
        print(f"   - Word: {docx_path}")
        
        # í´ë” ì—´ê¸° ì˜µì…˜
        print("\n" + "=" * 60)
        print("ğŸ“‚ ë¸”ë¡œê·¸ í´ë”ë¥¼ ì—¬ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ", end="")
        open_folder = input().strip().lower()
        if open_folder != 'n':
            subprocess.run(['explorer', str(WORD_OUTPUT_DIR)])
            print("   í´ë”ë¥¼ ì—´ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ë¸”ë¡œê·¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
